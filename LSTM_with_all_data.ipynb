{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from modules import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "review1=list(pd.read_csv(\"data/labeledTrainData.tsv\",sep = \"\\t\")['review'].apply(lambda x: x.strip().split(\" \")))\n",
    "review2=list(pd.read_csv(\"data/testData.tsv\",sep = \"\\t\")['review'].apply(lambda x: x.strip().split(\" \")))\n",
    "review3=list(pd.read_csv(\"data/unlabeledTrainData.tsv\",sep = \"\\t\",error_bad_lines=False)['review'].apply(lambda x: x.strip().split(\" \")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "99998"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data=review1+review2+review3\n",
    "len(all_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(review1+review2+review3, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
    "print(\"saving model ...\")\n",
    "#model.save(os.path.join(model, 'word2vec_all.model'))\n",
    "model.save(os.path.join('model', 'word2vec_all.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=os.path.join('model', 'word2vec_all.model')):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        # Load the previously trained word2vec model\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    # Add \"<PAD>\" or \"<UNK>\" to the embedding\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding,loading word2vec model ...\")\n",
    "        self.get_w2v_model()\n",
    "        # 製作一個 word2idx 的 dictionary\n",
    "        # 製作一個 idx2word 的 list\n",
    "        # 製作一個 word2vector 的 list\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            print('get words #{}'.format(i+1), end='\\r')\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "\n",
    "        # Add \"<PAD>\" and \"<UNK>\" to the embedding\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    # Make each sentence the same length\n",
    "    def pad_sequence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self):\n",
    "        # Convert the words in the sentence to the corresponding index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # Make each sentence the same length\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    def labels_to_tensor(self, y):\n",
    "        # labels --> tensor\n",
    "        y = [int(label) for label in y]\n",
    "\n",
    "        return torch.LongTensor(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        # Initialize the embedding using word2vec\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最後一層的 hidden state\n",
    "        x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data/labeledTrainData.tsv\",sep = \"\\t\")\n",
    "X = df['review'].apply(lambda x: x.strip().split(\" \"))\n",
    "X = list(X)\n",
    "y = df[\"sentiment\"]\n",
    "y = list(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding,loading word2vec model ...\n",
      "get words #121343\r\n",
      "total words: 121345\n",
      "sentence count #25000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 train  Loss:0.69300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 56.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 eval  Loss:0.68883 acc:0.51387\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 train  Loss:0.68484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 eval  Loss:0.69262 acc:0.49082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2 train  Loss:0.68941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 56.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2 eval  Loss:0.68375 acc:0.54141\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3 train  Loss:0.66117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 55.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3 eval  Loss:0.73079 acc:0.48535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4 train  Loss:0.68917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 55.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4 eval  Loss:0.68281 acc:0.55156\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5 train  Loss:0.68201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5 eval  Loss:0.69882 acc:0.49961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6 train  Loss:0.69025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 55.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6 eval  Loss:0.68805 acc:0.52207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7 train  Loss:0.68759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7 eval  Loss:0.68475 acc:0.53164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8 train  Loss:0.66320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8 eval  Loss:0.66751 acc:0.58574\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9 train  Loss:0.47980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9 eval  Loss:0.37166 acc:0.83633\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10 train  Loss:0.35628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10 eval  Loss:0.32936 acc:0.84199\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch11 train  Loss:0.31474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 52.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch11 eval  Loss:0.31045 acc:0.84648\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch12 train  Loss:0.28800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch12 eval  Loss:0.30386 acc:0.85215\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch13 train  Loss:0.27483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch13 eval  Loss:0.31414 acc:0.84570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch14 train  Loss:0.26194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch14 eval  Loss:0.29833 acc:0.85723\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch15 train  Loss:0.25169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch15 eval  Loss:0.29855 acc:0.85449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch16 train  Loss:0.24505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch16 eval  Loss:0.28906 acc:0.85820\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch17 train  Loss:0.22767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch17 eval  Loss:0.29629 acc:0.86426\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch18 train  Loss:0.21336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch18 eval  Loss:0.30776 acc:0.85430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch19 train  Loss:0.19761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 51.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch19 eval  Loss:0.31496 acc:0.85703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20 train  Loss:0.17817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 54.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20 eval  Loss:0.31255 acc:0.86035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch21 train  Loss:0.15899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch21 eval  Loss:0.34264 acc:0.85391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch22 train  Loss:0.13744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch22 eval  Loss:0.34760 acc:0.85684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch23 train  Loss:0.11378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 52.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch23 eval  Loss:0.38698 acc:0.85410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch24 train  Loss:0.10075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch24 eval  Loss:0.33420 acc:0.85703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch25 train  Loss:0.08609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch25 eval  Loss:0.44522 acc:0.84883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch26 train  Loss:0.06418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch26 eval  Loss:0.50048 acc:0.85313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch27 train  Loss:0.05149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 52.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch27 eval  Loss:0.48049 acc:0.84727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch28 train  Loss:0.04271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch28 eval  Loss:0.53973 acc:0.85293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch29 train  Loss:0.04298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 53.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch29 eval  Loss:0.53363 acc:0.84902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1\n",
    "    outputs[outputs<0.5] = 0\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return correct\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "preprocess = Preprocess(X, sen_len=200, w2v_path=os.path.join('model', 'word2vec_all.model'))\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "X = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "split_num = int(len(X)*0.8)\n",
    "X_train, X_val, y_train, y_val = X[:split_num], X[split_num:], y[:split_num], y[split_num:]\n",
    "\n",
    "# dataloader\n",
    "train_dataset = MyDataset(X=X_train, y=y_train)\n",
    "val_dataset = MyDataset(X=X_val, y=y_val)\n",
    "\n",
    "\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=1, dropout=0.3)\n",
    "\n",
    "\n",
    "batch=128\n",
    "train_loader=DataLoader(dataset = train_dataset,batch_size =batch,shuffle = True,num_workers = 0)\n",
    "val_loader=DataLoader(dataset = val_dataset,batch_size =batch,shuffle = True,num_workers = 0)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model=model.cuda()\n",
    "best_acc=0\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for i,(inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "         optimizer.zero_grad()\n",
    "         inputs = inputs.to(device, dtype=torch.long)\n",
    "         labels = labels.to(device, dtype=torch.float)\n",
    "         outputs = model(inputs)\n",
    "         outputs = outputs.squeeze()\n",
    "         loss = criterion(outputs, labels)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         total_loss += loss.item()\n",
    "\n",
    "    print(f'epoch{epoch} train ','Loss:{:.5f}'.format(total_loss/len(train_loader)))\n",
    "\n",
    "    #eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for i, (inputs, labels) in tqdm(enumerate(val_loader)):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = evaluation(outputs, labels)\n",
    "            total_acc += (correct / batch)\n",
    "            total_loss += loss.item()\n",
    "        print(f'epoch{epoch} eval ','Loss:{:.5f}'.format(total_loss/len(val_loader)),'acc:{:.5f}'.format(total_acc/len(val_loader)))\n",
    "        if total_acc/len(val_loader)>best_acc:\n",
    "            best_acc=total_acc/len(val_loader)\n",
    "            print('save')\n",
    "            torch.save(model, \"model/LSTM_all_data.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding,loading word2vec model ...\n",
      "get words #121343\r\n",
      "total words: 121345\n",
      "sentence count #25000\r\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "test_data=pd.read_csv(\"data/testData.tsv\",sep = \"\\t\")\n",
    "X = test_data['review'].apply(lambda x: x.strip().split(\" \"))\n",
    "X = list(X)\n",
    "preprocess = Preprocess(X, sen_len=200)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = MyDataset(X=test_x, y=None)\n",
    "test_loader = DataLoader(dataset = test_dataset,batch_size =batch,shuffle = False,num_workers = 0)\n",
    "print('\\nload model ...')\n",
    "model = torch.load(os.path.join('model', 'LSTM_all_data.model'))\n",
    "model.eval()\n",
    "ret_output = []\n",
    "with torch.no_grad():\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.to(device, dtype=torch.long)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        ret_output += outputs.float().tolist()\n",
    "tmp = pd.DataFrame({\"id\":test_data['id'],\"sentiment\":ret_output})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join('out', 'LSTM_all.csv'), index=False)\n",
    "print(\"Finish Predicting\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}